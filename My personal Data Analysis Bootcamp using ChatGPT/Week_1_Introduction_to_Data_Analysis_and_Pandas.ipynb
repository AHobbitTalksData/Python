{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437b0253",
   "metadata": {},
   "source": [
    "# PANDAS (part 1)\n",
    "\n",
    " You will learn the basics of data analysis and get familiar with the Pandas library, which is widely used \n",
    "for data manipulation and analysis in Python.\n",
    "\n",
    "### Objectives \n",
    "\n",
    "-Reading and writing data: Learn how to read data from various file formats (e.g., CSV, Excel) using Pandas and write data \n",
    "to files.\n",
    "\n",
    "-Data cleaning: Explore techniques\n",
    "to handle missing values, handle duplicates, and perform data type conversions.\n",
    "\n",
    "-Data exploration: Use Pandas functions to gain insights into the data, such as descriptive statistics, value counts, and \n",
    " data profiling.\n",
    "\n",
    "\n",
    "### Project 1: Data Cleaning and Exploration \n",
    " Instructions: Choose a dataset of your choice (e.g., CSV file) and perform data cleaning tasks such as handling missing \n",
    "values, removing duplicates, and transforming data types. Explore the dataset using Pandas functions to gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec97fa",
   "metadata": {},
   "source": [
    "## 1.  Reading and Importing data using Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6a84a",
   "metadata": {},
   "source": [
    "### 1.1 Reading files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524eb3a",
   "metadata": {},
   "source": [
    "Pandas provides various functions and methods to read data from different file formats. Here's the necessary information for reading different formats with Pandas:\n",
    "\n",
    "- CSV (Comma-Separated Values):\n",
    "\n",
    "read_csv(): This function reads data from a CSV file into a DataFrame. It supports various parameters to handle different file configurations such as delimiter, header row, column names, and more.\n",
    "\n",
    "- Excel:\n",
    "\n",
    "read_excel(): This function reads data from an Excel file into a DataFrame. It supports parameters to specify sheet names or sheet indices, skiprows, column names, and more.\n",
    "\n",
    "- JSON (JavaScript Object Notation):\n",
    "\n",
    "read_json(): This function reads data from a JSON file into a DataFrame. It supports parameters to handle different JSON file structures, such as orient, lines, and more.\n",
    "\n",
    "- SQL (Structured Query Language) Database:\n",
    "\n",
    "read_sql(): This function reads data from a SQL database query result directly into a DataFrame. It requires a database connection and accepts parameters such as SQL query, connection object, and more.\n",
    "\n",
    "- Other File Formats:\n",
    "\n",
    "Pandas also supports reading data from other formats such as Excel files with multiple sheets, HTML tables, SAS files, Stata files, and more. For each specific format, Pandas provides dedicated functions or methods. You can refer to the Pandas documentation for the complete list of supported file formats and the corresponding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be40b37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Read data from a CSV file\\ndf_csv = pd.read_csv('data.csv')\\n\\n# Read data from an Excel file\\ndf_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1')\\n\\n# Read data from a JSON file\\ndf_json = pd.read_json('data.json')\\n\\n# Read data from a SQL database\\nimport sqlite3\\nconn = sqlite3.connect('database.db')\\nquery = 'SELECT * FROM table_name'\\ndf_sql = pd.read_sql(query, conn)\\nconn.close()\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "# Read data from a CSV file\n",
    "df_csv = pd.read_csv('data.csv')\n",
    "\n",
    "# Read data from an Excel file\n",
    "df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Read data from a JSON file\n",
    "df_json = pd.read_json('data.json')\n",
    "\n",
    "# Read data from a SQL database\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('database.db')\n",
    "query = 'SELECT * FROM table_name'\n",
    "df_sql = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15853ba8",
   "metadata": {},
   "source": [
    "### 1.2 Importing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43802406",
   "metadata": {},
   "source": [
    " Here's the necessary information for importing different formats with Pandas:\n",
    "\n",
    "- CSV (Comma-Separated Values):\n",
    "\n",
    "pd.DataFrame(): You can create a DataFrame directly from CSV data by passing the CSV data as a list of lists or a dictionary.\n",
    "\n",
    "- Excel:\n",
    "\n",
    "pd.read_excel(): This function reads data from an Excel file into a DataFrame. It supports parameters to specify sheet names or sheet indices, skiprows, column names, and more.\n",
    "\n",
    "- JSON (JavaScript Object Notation):\n",
    "\n",
    "pd.read_json(): This function reads data from a JSON file into a DataFrame. It supports parameters to handle different JSON file structures, such as orient, lines, and more.\n",
    "\n",
    "- SQL (Structured Query Language) Database:\n",
    "\n",
    "pd.read_sql_query(): This function reads data from a SQL database query result directly into a DataFrame. It requires a database connection and accepts parameters such as SQL query and connection object.\n",
    "Other File Formats:\n",
    "\n",
    "- Pandas supports importing data from various other formats such as Excel files with multiple sheets, HTML tables, SAS files, Stata files, and more. For each specific format, Pandas provides dedicated functions or methods. You can refer to the Pandas documentation for the complete list of supported file formats and the corresponding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f900e231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Import data from a CSV file\\ndf_csv = pd.DataFrame([\\n    ['John', 28, 'New York'],\\n    ['Emma', 35, 'Chicago'],\\n    ['Oliver', 42, 'Seattle']\\n], columns=['Name', 'Age', 'City'])\\n\\n# Import data from an Excel file\\ndf_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1')\\n\\n# Import data from a JSON file\\ndf_json = pd.read_json('data.json')\\n\\n# Import data from a SQL database\\nimport sqlite3\\nconn = sqlite3.connect('database.db')\\nquery = 'SELECT * FROM table_name'\\ndf_sql = pd.read_sql_query(query, conn)\\nconn.close()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "# Import data from a CSV file\n",
    "df_csv = pd.DataFrame([\n",
    "    ['John', 28, 'New York'],\n",
    "    ['Emma', 35, 'Chicago'],\n",
    "    ['Oliver', 42, 'Seattle']\n",
    "], columns=['Name', 'Age', 'City'])\n",
    "\n",
    "# Import data from an Excel file\n",
    "df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Import data from a JSON file\n",
    "df_json = pd.read_json('data.json')\n",
    "\n",
    "# Import data from a SQL database\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('database.db')\n",
    "query = 'SELECT * FROM table_name'\n",
    "df_sql = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f115dd0",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7050b43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\\'data.csv\\', index_col=0, parse_dates=[\\'IND_DAY\\'])\\ndf.to_csv(\\'formatted-data.csv\\', date_format=\\'%B %d, %Y\\')\\ns = df.to_csv(sep=\\';\\', header=False) # the data is separated by \";\" and the saved file will not have headers \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dtypes = {'POP': 'float32', 'AREA': 'float32', 'GDP': 'float32'}\n",
    "df = pd.read_csv('data.csv', index_col=0, dtype=dtypes, parse_dates=['IND_DAY'])\n",
    "\"\"\"\n",
    "\n",
    "# Saving in a specific format \n",
    "\"\"\"\n",
    "df = pd.read_csv('data.csv', index_col=0, parse_dates=['IND_DAY'])\n",
    "df.to_csv('formatted-data.csv', date_format='%B %d, %Y')\n",
    "s = df.to_csv(sep=';', header=False) # the data is separated by \";\" and the saved file will not have headers \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c704ed3",
   "metadata": {},
   "source": [
    "### 1.4 Other object creation options\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41569c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf2.dtypes \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Series by passing a list of values, letting pandas create a default integer index:\n",
    "\"\"\"\n",
    "s = pd.Series([1, 3, 5, np.nan, 6, 8]) \n",
    "\"\"\"\n",
    "\n",
    "# Creating a DataFrame by passing a NumPy array, with a datetime index using date_range() and labeled columns:\n",
    "\"\"\"\n",
    "dates = pd.date_range(\"20130101\", periods=6)\n",
    "----\n",
    "DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n",
    "               '2013-01-05', '2013-01-06'],\n",
    "              dtype='datetime64[ns]', freq='D')\n",
    "----\n",
    "df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\n",
    "\"\"\"\n",
    "\n",
    "# Creating a DataFrame by passing a dictionary of objects that can be converted into a series-like structure:\n",
    "\"\"\"\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": 1.0,\n",
    "        \"B\": pd.Timestamp(\"20130102\"),\n",
    "        \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"),\n",
    "        \"D\": np.array([3] * 4, dtype=\"int32\"),\n",
    "        \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n",
    "        \"F\": \"foo\",\n",
    "    }\n",
    ") \n",
    "\"\"\"\n",
    "\n",
    "# The columns of the resulting DataFrame have different dtypes:\n",
    "\"\"\"\n",
    "df2.dtypes \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e47f947",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1338891434.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [9]\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\"\u001b[0m\n\u001b[1;37m        \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# List of attributes: \n",
    "\n",
    "\"\"\"\"\n",
    "df2.A                  df2.bool\n",
    "df2.abs                df2.boxplot\n",
    "df2.add                df2.C\n",
    "df2.add_prefix         df2.clip\n",
    "df2.add_suffix         df2.columns\n",
    "df2.align              df2.copy\n",
    "df2.all                df2.count\n",
    "df2.any                df2.combine\n",
    "df2.append             df2.D\n",
    "df2.apply              df2.describe\n",
    "df2.applymap           df2.diff\n",
    "df2.B                  df2.duplicated\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0509a",
   "metadata": {},
   "source": [
    "## 2. Data cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031ad3d",
   "metadata": {},
   "source": [
    "### 2.1  Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd2489",
   "metadata": {},
   "source": [
    "- Pandas uses np.nan to represent missing data. It is not included in calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941fcd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npd.isna(df1) \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Reindexing allows to change/add/delete the index of a specified axis. It returns a copy of the data. \n",
    "\"\"\"\n",
    "df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [\"E\"])\n",
    "df1.loc[dates[0] : dates[1], \"E\"] = 1\n",
    "\"\"\"\n",
    "\n",
    "# DataFrame.dropna() drops any rows that have missing data:\n",
    "\"\"\"\n",
    "df1.dropna(how=\"any\") \n",
    "\"\"\"\n",
    "\n",
    "# DataFrame.fillna() fills missing data:\n",
    "\"\"\"\n",
    "df1.fillna(value=5) \n",
    "\"\"\"\n",
    "\n",
    "# isna() gets the boolean mask where values are nan:\n",
    "\"\"\"\n",
    "pd.isna(df1) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83fb1def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "    Name    Age   City\n",
      "0  False  False  False\n",
      "1  False   True  False\n",
      "2   True  False   True\n",
      "3  False   True  False\n",
      "DataFrame after Dropping Missing Values:\n",
      "   Name   Age      City\n",
      "0  John  28.0  New York\n",
      "DataFrame after Filling Missing Values:\n",
      "      Name      Age      City\n",
      "0     John     28.0  New York\n",
      "1     Emma  Unknown   Chicago\n",
      "2  Unknown     42.0   Unknown\n",
      "3   Oliver  Unknown   Seattle\n"
     ]
    }
   ],
   "source": [
    "#Example \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {'Name': ['John', 'Emma', np.nan, 'Oliver'],\n",
    "        'Age': [28, np.nan, 42, np.nan],\n",
    "        'City': ['New York', 'Chicago', np.nan, 'Seattle']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Detect missing values\n",
    "missing_values = df.isnull()\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_dropped = df.dropna()\n",
    "print(\"DataFrame after Dropping Missing Values:\")\n",
    "print(df_dropped)\n",
    "\n",
    "# Fill missing values with a specified value\n",
    "df_filled = df.fillna(value='Unknown')\n",
    "print(\"DataFrame after Filling Missing Values:\")\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75a767",
   "metadata": {},
   "source": [
    "### 2.2 Handling duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13e082",
   "metadata": {},
   "source": [
    "Here's an overview of the essential information you need to handle duplicate values in Pandas:\n",
    "\n",
    "1. Detecting Duplicate Values:\n",
    "\n",
    "- duplicated(): This function identifies duplicate rows in a DataFrame and returns a Boolean series indicating which rows are duplicates.\n",
    "- drop_duplicates(): This method removes duplicate rows from a DataFrame and returns a new DataFrame without duplicates.\n",
    "\n",
    "2. Handling Duplicate Values:\n",
    "\n",
    "- keep parameter: Both duplicated() and drop_duplicates() accept a keep parameter that determines which duplicates to keep or remove.\n",
    "- keep='first' (default): Keeps the first occurrence of each duplicated row and removes the subsequent duplicates.\n",
    "-keep='last': Keeps the last occurrence of each duplicated row and removes the previous duplicates.\n",
    "- keep=False: Removes all occurrences of duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "289b9df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "   Name  Age      City\n",
      "2  John   28  New York\n",
      "4  Emma   35   Chicago\n",
      "DataFrame without Duplicates:\n",
      "     Name  Age      City\n",
      "0    John   28  New York\n",
      "1    Emma   35   Chicago\n",
      "3  Oliver   42   Seattle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with duplicate values\n",
    "data = {'Name': ['John', 'Emma', 'John', 'Oliver', 'Emma'],\n",
    "        'Age': [28, 35, 28, 42, 35],\n",
    "        'City': ['New York', 'Chicago', 'New York', 'Seattle', 'Chicago']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Detect duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "print(\"Duplicate Rows:\")\n",
    "print(df[duplicates])\n",
    "\n",
    "# Drop duplicate rows and keep the first occurrence\n",
    "df_unique = df.drop_duplicates(keep='first')\n",
    "print(\"DataFrame without Duplicates:\")\n",
    "print(df_unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892aef00",
   "metadata": {},
   "source": [
    "### 2.3 Performing data type conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744d29a",
   "metadata": {},
   "source": [
    " Here's the necessary information for performing data type conversions in Pandas:\n",
    "\n",
    "1. Conversion to Numeric Types:\n",
    "\n",
    "pd.to_numeric(): This function converts a column to a numeric type. It can handle different data types and options for handling errors and converting non-numeric values.\n",
    "\n",
    "2. Conversion to DateTime Types:\n",
    "\n",
    "pd.to_datetime(): This function converts a column to a DateTime type. It can handle various date and time formats and options for handling errors and parsing specific components.\n",
    "\n",
    "3. Conversion to Categorical Types:\n",
    "\n",
    "astype(): The astype() method can be used to convert a column to a Categorical type, which is useful for working with categorical or nominal data. Specify the data type as 'category'.\n",
    "\n",
    "4. Conversion to String Types:\n",
    "\n",
    "astype(): The astype() method can be used to convert a column to a string type, which is useful for performing string operations. Specify the data type as 'string'.\n",
    "\n",
    "5. Conversion to Boolean Types:\n",
    "\n",
    "astype(): The astype() method can be used to convert a column to a boolean type, which is useful for working with logical values. Specify the data type as 'bool'.\n",
    "\n",
    "6. Conversion to Other Numeric Types:\n",
    "\n",
    "astype(): The astype() method can be used to convert a column to other numeric types, such as int, float, int64, etc. Specify the desired data type accordingly.\n",
    "\n",
    "7. Conversion to Categorical Ordinal Types:\n",
    "\n",
    "pd.Categorical(): This function can be used to convert a column to a Categorical type with an order or specified categories. It is useful for working with ordinal data. Specify the data type as 'category' and provide the categories parameter.\n",
    "\n",
    "8. Conversion to Timedelta Types:\n",
    "\n",
    "pd.to_timedelta(): This function converts a column to a Timedelta type, which represents a duration or difference between two dates or times. It can handle different time units and options for error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b23635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nimport pandas as pd\\n\\n# Convert a column to numeric type\\ndf['Column'] = pd.to_numeric(df['Column'], errors='coerce')\\n\\n# Convert a column to DateTime type\\ndf['Column'] = pd.to_datetime(df['Column'], format='%Y-%m-%d')\\n\\n# Convert a column to Categorical type\\ndf['Column'] = df['Column'].astype('category')\\n\\n# Convert a column to string type\\ndf['Column'] = df['Column'].astype('string')\\n\\n# Convert a column to boolean type\\ndf['Column'] = df['Column'].astype('bool')\\n\\n# Convert a column to other numeric types\\ndf['Column'] = df['Column'].astype(int)\\n\\n# Convert a column to Categorical ordinal type\\ndf['Column'] = pd.Categorical(df['Column'], categories=['Low', 'Medium', 'High'], ordered=True)\\n\\n# Convert a column to Timedelta type\\ndf['Column'] = pd.to_timedelta(df['Column'], unit='days')\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert a column to numeric type\n",
    "df['Column'] = pd.to_numeric(df['Column'], errors='coerce')\n",
    "\n",
    "# Convert a column to DateTime type\n",
    "df['Column'] = pd.to_datetime(df['Column'], format='%Y-%m-%d')\n",
    "\n",
    "# Convert a column to Categorical type\n",
    "df['Column'] = df['Column'].astype('category')\n",
    "\n",
    "# Convert a column to string type\n",
    "df['Column'] = df['Column'].astype('string')\n",
    "\n",
    "# Convert a column to boolean type\n",
    "df['Column'] = df['Column'].astype('bool')\n",
    "\n",
    "# Convert a column to other numeric types\n",
    "df['Column'] = df['Column'].astype(int)\n",
    "\n",
    "# Convert a column to Categorical ordinal type\n",
    "df['Column'] = pd.Categorical(df['Column'], categories=['Low', 'Medium', 'High'], ordered=True)\n",
    "\n",
    "# Convert a column to Timedelta type\n",
    "df['Column'] = pd.to_timedelta(df['Column'], unit='days')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586b1c0",
   "metadata": {},
   "source": [
    "## 3. Data exploration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773276b7",
   "metadata": {},
   "source": [
    "### 3.1 Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b074d",
   "metadata": {},
   "source": [
    "Here are some of the basic functions for descriptive statistics: \n",
    "\n",
    "- describe(): This function computes various descriptive statistics of each numerical column in a DataFrame, including count, mean, standard deviation, minimum, maximum, and quartile values. By default, it provides statistics for numeric columns only.\n",
    "- mean(): This function calculates the mean of each numerical column in a DataFrame.\n",
    "- median(): This function calculates the median (50th percentile) of each numerical column.\n",
    "- std(): This function calculates the standard deviation of each numerical column.\n",
    "- min(): This function returns the minimum value of each numerical column.\n",
    "- max(): This function returns the maximum value of each numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be190d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics:\n",
      "            Age\n",
      "count   5.00000\n",
      "mean   33.60000\n",
      "std     5.85662\n",
      "min    28.00000\n",
      "25%    28.00000\n",
      "50%    35.00000\n",
      "75%    35.00000\n",
      "max    42.00000\n",
      "Mean of Columns:\n",
      "Age    33.6\n",
      "dtype: float64\n",
      "Median of Columns:\n",
      "Age    35.0\n",
      "dtype: float64\n",
      "Standard Deviation of Columns:\n",
      "Age    5.85662\n",
      "dtype: float64\n",
      "Minimum Values of Columns:\n",
      "Name       Emma\n",
      "Age          28\n",
      "City    Chicago\n",
      "dtype: object\n",
      "Maximum Values of Columns:\n",
      "Name     Oliver\n",
      "Age          42\n",
      "City    Seattle\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pame\\AppData\\Local\\Temp\\ipykernel_7944\\1781865728.py:11: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  column_means = df.mean()\n",
      "C:\\Users\\Pame\\AppData\\Local\\Temp\\ipykernel_7944\\1781865728.py:16: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  column_medians = df.median()\n",
      "C:\\Users\\Pame\\AppData\\Local\\Temp\\ipykernel_7944\\1781865728.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  column_std = df.std()\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate descriptive statistics of the DataFrame\n",
    "descriptive_stats = df.describe()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(descriptive_stats)\n",
    "\n",
    "# Calculate the mean of each numerical column\n",
    "column_means = df.mean()\n",
    "print(\"Mean of Columns:\")\n",
    "print(column_means)\n",
    "\n",
    "# Calculate the median of each numerical column\n",
    "column_medians = df.median()\n",
    "print(\"Median of Columns:\")\n",
    "print(column_medians)\n",
    "\n",
    "# Calculate the standard deviation of each numerical column\n",
    "column_std = df.std()\n",
    "print(\"Standard Deviation of Columns:\")\n",
    "print(column_std)\n",
    "\n",
    "# Get the minimum value of each numerical column\n",
    "column_min = df.min()\n",
    "print(\"Minimum Values of Columns:\")\n",
    "print(column_min)\n",
    "\n",
    "# Get the maximum value of each numerical column\n",
    "column_max = df.max()\n",
    "print(\"Maximum Values of Columns:\")\n",
    "print(column_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8c2bd",
   "metadata": {},
   "source": [
    "### 3.2 Value counts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ffa6b",
   "metadata": {},
   "source": [
    "Here's the necessary information for using value counts in Pandas:\n",
    "\n",
    "1. Value Counts Function:\n",
    "\n",
    "value_counts(): This function returns a Series containing counts of unique values in a column. By default, it sorts the counts in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07f111be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport pandas as pd\\n\\n# Calculate value counts for a column\\nvalue_counts = df[\\'Column\\'].value_counts()\\nprint(\"Value Counts:\")\\nprint(value_counts)\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate value counts for a column\n",
    "value_counts = df['Column'].value_counts()\n",
    "print(\"Value Counts:\")\n",
    "print(value_counts)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f876e",
   "metadata": {},
   "source": [
    "#### 3.2.1 Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ad03a",
   "metadata": {},
   "source": [
    "Here's the necessary information for performing operations in Pandas:\n",
    "\n",
    "1. Applying Functions to Columns or Rows:\n",
    "\n",
    "apply(): This function applies a function along either the rows or columns of a DataFrame. You can pass a built-in or custom function to perform a specific operation on each element or group of elements.\n",
    "\n",
    "applymap(): This method applies a function to every element of a DataFrame. It is useful for element-wise operations.\n",
    "\n",
    "2. Arithmetic Operations:\n",
    "\n",
    "Arithmetic operators (+, -, *, /, **, etc.): These operators allow you to perform element-wise arithmetic operations on columns or rows of a DataFrame. The operations can be between columns or between columns and scalar values.\n",
    "\n",
    "add(), sub(), mul(), div(): These methods perform element-wise addition, subtraction, multiplication, and division between two DataFrames or between a DataFrame and a scalar.\n",
    "\n",
    "3. Groupby Operations:\n",
    "\n",
    "groupby(): This function is used to group data based on one or more columns. It allows you to apply aggregate functions such as sum(), mean(), count(), etc. to each group.\n",
    "Sorting Data:\n",
    "\n",
    "sort_values(): This function sorts the rows of a DataFrame based on one or more columns.\n",
    "sort_index(): This function sorts the rows of a DataFrame based on the row index.\n",
    "\n",
    "4. Merging Data:\n",
    "\n",
    "merge(): This function combines two or more DataFrames based on common columns or indices. It supports different types of joins, such as inner join, outer join, left join, and right join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f6aa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\n\\n# Apply a function to a column\\ndf['Column'] = df['Column'].apply(lambda x: x + 1)\\n\\n# Apply a function to every element of a DataFrame\\ndf = df.applymap(lambda x: x.lower())\\n\\n# Perform arithmetic operations between columns\\ndf['Result'] = df['Column1'] + df['Column2']\\n\\n# Perform arithmetic operations between a column and a scalar\\ndf['Result'] = df['Column'] * 2\\n\\n# Group data and apply aggregate functions\\ngrouped_data = df.groupby('GroupColumn')['NumericColumn'].sum()\\n\\n# Sort the DataFrame by a column\\ndf = df.sort_values(by='Column')\\n\\n# Sort the DataFrame by the row index\\ndf = df.sort_index()\\n\\n# Merge two DataFrames based on a common column\\nmerged_df = pd.merge(df1, df2, on='CommonColumn', how='inner')\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Apply a function to a column\n",
    "df['Column'] = df['Column'].apply(lambda x: x + 1)\n",
    "\n",
    "# Apply a function to every element of a DataFrame\n",
    "df = df.applymap(lambda x: x.lower())\n",
    "\n",
    "# Perform arithmetic operations between columns\n",
    "df['Result'] = df['Column1'] + df['Column2']\n",
    "\n",
    "# Perform arithmetic operations between a column and a scalar\n",
    "df['Result'] = df['Column'] * 2\n",
    "\n",
    "# Group data and apply aggregate functions\n",
    "grouped_data = df.groupby('GroupColumn')['NumericColumn'].sum()\n",
    "\n",
    "# Sort the DataFrame by a column\n",
    "df = df.sort_values(by='Column')\n",
    "\n",
    "# Sort the DataFrame by the row index\n",
    "df = df.sort_index()\n",
    "\n",
    "# Merge two DataFrames based on a common column\n",
    "merged_df = pd.merge(df1, df2, on='CommonColumn', how='inner')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1392ce1",
   "metadata": {},
   "source": [
    "### 3.3 Data profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e2197",
   "metadata": {},
   "source": [
    "Here are some key components and functions in Pandas that can help you perform data profiling:\n",
    "\n",
    "1. Data Shape and Structure:\n",
    "\n",
    "shape: This attribute provides the dimensions of your DataFrame, i.e., the number of rows and columns.\n",
    "dtypes: This attribute displays the data types of each column in your DataFrame.\n",
    "\n",
    "2. Data Summary:\n",
    "\n",
    "info(): This function provides a concise summary of your DataFrame, including the number of non-null values and the data types of each column.\n",
    "head(): This function displays the first few rows of your DataFrame, giving you a glimpse of the data.\n",
    "\n",
    "3. Missing Values:\n",
    "\n",
    "isnull(): This function checks for missing or null values in your DataFrame and returns a boolean mask.\n",
    "sum(): This function can be used with isnull() to count the number of missing values in each column.\n",
    "fillna(): This function allows you to fill missing values with a specified value or strategy.\n",
    "\n",
    "4. Unique Values:\n",
    "\n",
    "nunique(): This function calculates the number of unique values in each column of your DataFrame.\n",
    "unique(): This function returns an array of unique values in a specific column.\n",
    "\n",
    "5. Data Distribution:\n",
    "\n",
    "hist(): This function can be used to visualize the distribution of numerical data through histograms.\n",
    "value_counts(): This function, as discussed earlier, provides the count of unique values in a column.\n",
    "\n",
    "6. Correlation Analysis:\n",
    "\n",
    "corr(): This function calculates the pairwise correlation between columns in your DataFrame, allowing you to identify relationships between variables.\n",
    "\n",
    "7. Summary Statistics:\n",
    "\n",
    "describe(): This function provides a comprehensive summary of your DataFrame's numeric columns, including count, mean, standard deviation, quartiles, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30b6266f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport pandas as pd\\n\\n# Check the shape of the DataFrame\\nprint(\"Data Shape:\")\\nprint(df.shape)\\n\\n# Display data types of columns\\nprint(\"Data Types:\")\\nprint(df.dtypes)\\n\\n# Display a concise summary of the DataFrame\\nprint(\"Data Summary:\")\\nprint(df.info())\\n\\n# Display the first few rows of the DataFrame\\nprint(\"First Few Rows:\")\\nprint(df.head())\\n\\n# Check for missing values\\nprint(\"Missing Values:\")\\nprint(df.isnull().sum())\\n\\n# Count unique values in a column\\nprint(\"Unique Values:\")\\nprint(df[\\'Column\\'].nunique())\\n\\n# Calculate summary statistics\\nprint(\"Summary Statistics:\")\\nprint(df.describe())\\n\\n# Visualize the distribution of a column\\ndf[\\'Column\\'].hist()\\n \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Check the shape of the DataFrame\n",
    "print(\"Data Shape:\")\n",
    "print(df.shape)\n",
    "\n",
    "# Display data types of columns\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display a concise summary of the DataFrame\n",
    "print(\"Data Summary:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First Few Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Count unique values in a column\n",
    "print(\"Unique Values:\")\n",
    "print(df['Column'].nunique())\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Visualize the distribution of a column\n",
    "df['Column'].hist()\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1409d62",
   "metadata": {},
   "source": [
    "## Project 1: Data Cleaning and Exploration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0ca92",
   "metadata": {},
   "source": [
    "Instructions \n",
    "\n",
    "Choose a dataset of your choice (e.g., CSV file) and perform data cleaning tasks such as handling missing values, removing duplicates, and transforming data types. Explore the dataset using Pandas functions to gain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e5135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
